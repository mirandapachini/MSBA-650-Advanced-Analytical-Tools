{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# In Google Colab, spaCy may not be installed by default.\n",
        "!pip -q install nltk spacy\n",
        "# spaCy models (like en_core_web_sm) are separate downloads.\n",
        "!python -m spacy download en_core_web_sm -q\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "# Counter is a dictionary-like class that counts occurrences of items.\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "# spacy.load loads a trained pipeline: tokenizer + tagger + lemmatizer, etc.\n",
        "# \"en_core_web_sm\" is the small English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print('✅ All libraries loaded successfully!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P8wFFWxXpNo",
        "outputId": "81ef153e-6205-4fe7-d1e5-b3e29eb71f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/12.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/12.8 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/12.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m10.3/12.8 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m197.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "✅ All libraries loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:"
      ],
      "metadata": {
        "id": "N_J15ZzzBFFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"I love the battery life, it lasts for days!\",\n",
        "    \"The battery is NOT charging. I am very angry.\",\n",
        "    \"Charger broke after one week. Unacceptable quality.\",\n",
        "    \"Best smartwatch ever. Tracks my running and sleeping perfectly.\",\n",
        "    \"The run tracking is inaccurate and the sleep tracker is worse.\",\n",
        "    \"Touchscreen is lagging and slow.\",\n",
        "    \"I was waiting for a refund for two weeks.\",\n",
        "    \"Don't buy this. It is a waste of money.\",\n",
        "    \"Amazing features but the strap is uncomfortable.\",\n",
        "    \"Support helped me fix the sync issue. Friendly service!\"\n",
        "]\n",
        "\n",
        "# Let's look at the first review we'll use as our running example\n",
        "example = reviews[4]\n",
        "print(\"Our example review:\")\n",
        "print(f'  \"{example}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn4vjPmiXpa5",
        "outputId": "35e1f5a2-2679-40d2-bdf0-f0717f367cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our example review:\n",
            "  \"The run tracking is inaccurate and the sleep tracker is worse.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses NLTK’s tokenizer.\n",
        "tokens = word_tokenize(example)\n",
        "\n",
        "print(f\"Original text  ({len(example.split())} words):\")\n",
        "print(f'  \"{example}\"\\n')\n",
        "\n",
        "print(f\"Tokens ({len(tokens)} total):\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMqArxX5XpiP",
        "outputId": "18000987-9cd4-473e-b302-306a2bf499a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text  (11 words):\n",
            "  \"The run tracking is inaccurate and the sleep tracker is worse.\"\n",
            "\n",
            "Tokens (12 total):\n",
            "['The', 'run', 'tracking', 'is', 'inaccurate', 'and', 'the', 'sleep', 'tracker', 'is', 'worse', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))  # #Converts list to set for fast membership checks.\n",
        "\n",
        "# Keep only words that are alphabetic and NOT stopwords\n",
        "filtered_tokens = [\n",
        "    word.lower() for word in tokens\n",
        "    if word.isalpha() and word.lower() not in stop_words # careful about isalpha\n",
        "]\n",
        "\n",
        "print(\"After removing stopwords and punctuation:\")\n",
        "print(filtered_tokens)\n",
        "print(f\"\\nWent from {len(tokens)} tokens → {len(filtered_tokens)} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8uS49tkXpw0",
        "outputId": "36307a16-9238-42af-fe8b-650baebe77e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing stopwords and punctuation:\n",
            "['run', 'tracking', 'inaccurate', 'sleep', 'tracker', 'worse']\n",
            "\n",
            "Went from 12 tokens → 6 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# which words are considered stopwords? Here's a sample:\n",
        "sample_stopwords = sorted(list(stop_words))[:30]\n",
        "print(\"Sample stopwords (first 30):\")\n",
        "print(sample_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AdZ0EBZXqAJ",
        "outputId": "b717c9db-ba5e-4529-c02b-ed06f2bf8c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample stopwords (first 30):\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"Original → Stemmed:\")\n",
        "for orig, stem in zip(filtered_tokens, stemmed):\n",
        "    print(f\"  {orig:15s} → {stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlxg0BZTXqHf",
        "outputId": "0073a659-5e89-4e3e-bf8d-fbb01fec3458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original → Stemmed:\n",
            "  love            → love\n",
            "  battery         → batteri\n",
            "  life            → life\n",
            "  lasts           → last\n",
            "  days            → day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:"
      ],
      "metadata": {
        "id": "7vai1GPzBDmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_distance(s1, s2):\n",
        "    \"\"\"Compute the Levenshtein edit distance between two strings.\"\"\"\n",
        "    m, n = len(s1), len(s2)\n",
        "    # Build a (m+1) x (n+1) matrix\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Base cases: transforming to/from empty string\n",
        "    for i in range(m + 1): dp[i][0] = i  # delete all chars\n",
        "    for j in range(n + 1): dp[0][j] = j  # insert all chars\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if s1[i-1] == s2[j-1]:         # characters match — no cost\n",
        "                dp[i][j] = dp[i-1][j-1]\n",
        "            else:                            # pick cheapest operation\n",
        "                dp[i][j] = 1 + min(\n",
        "                    dp[i-1][j],    # deletion\n",
        "                    dp[i][j-1],    # insertion\n",
        "                    dp[i-1][j-1]   # substitution\n",
        "                )\n",
        "    return dp[m][n]"
      ],
      "metadata": {
        "id": "izT-PvfpXqXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Typos and misspellings\n",
        "word_pairs = [\n",
        "    (\"algorithm\",\"logarithm\")\n",
        "]\n",
        "\n",
        "print(f\"{'Word 1':<12} {'Word 2':<12} {'Edit Distance':<15} {'Interpretation'}\")\n",
        "print(\"-\" * 60)\n",
        "for w1, w2 in word_pairs:\n",
        "    d = edit_distance(w1, w2)\n",
        "    interp = \"very similar\" if d <= 1 else (\"similar\" if d <= 2 else \"different\")\n",
        "    print(f\"{w1:<12} {w2:<12} {d:<15} {interp}\")"
      ],
      "metadata": {
        "id": "PFDuoUpvXqgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddcad37-09ae-4b7e-db7c-342870906891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 1       Word 2       Edit Distance   Interpretation\n",
            "------------------------------------------------------------\n",
            "algorithm    logarithm    3               different\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:"
      ],
      "metadata": {
        "id": "Iqr8D4PoBftU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary (ordered)\n",
        "\n",
        "vocabulary = [\"wireless\", \"noise\", \"cancelling\", \"headphones\", \"with\", \"active\", \"cancellation\", \"wired\", \"over-ear\", \"studio\", \"earbuds\",\n",
        "\"reduction\"]\n",
        "\n",
        "# Query and Documents\n",
        "\n",
        "q_text= \"wireless noise cancelling headphones\"\n",
        "d1_text= \"wireless headphones with active noise cancellation\"\n",
        "d2_text= \"wired over-ear studio headphones\"\n",
        "d3_text= \"wireless earbuds with noise reduction\"\n",
        "\n",
        "documents=[d1_text, d2_text, d3_text]\n",
        "documents_names=[\"d1\", \"d2\", \"d3\"]\n",
        "docs=dict(zip(documents_names, documents))\n",
        "\n",
        "query=q_text\n",
        "\n",
        "# Function to convert text to BoW vector\n",
        "\n",
        "def bow_vector(text, vocabulary):\n",
        "  words=text.lower().split()\n",
        "  return[words.count(term) for term in vocabulary]\n",
        "\n",
        "#create vectors\n",
        "\n",
        "q_vector=bow_vector(q_text, vocabulary)\n",
        "d_vectors=[bow_vector(doc, vocabulary) for doc in documents]\n",
        "\n",
        "print(\"Query vector:\", q_vector)\n",
        "for name, vector in zip(documents_names, d_vectors):\n",
        "  print(f\"{name} —> {vector}\")\n",
        "\n",
        "# Compute dot product similarity\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "scores=[np.dot(q_vector, d_vector) for d_vector in d_vectors]\n",
        "\n",
        "print(\"\\nSimilarity Scores:\")\n",
        "for name, score in zip(documents_names, scores):\n",
        "  print(f\"{name} —> {score}\")\n",
        "\n",
        "best_match=documents_names[np.argmax(scores)]\n",
        "print(\"\\nBest Matching Document:\", best_match)\n",
        "print(\"Best Match Text:\", docs[best_match])\n",
        "\n",
        "# AI Resourses: Microsoft Copilot and Gemini\n"
      ],
      "metadata": {
        "id": "I4kgFQrLXqo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f5fb5d-4b9b-4ced-f1b2-7bfbe9f0d18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vector: [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "d1 —> [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
            "d2 —> [0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
            "d3 —> [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "\n",
            "Similarity Scores:\n",
            "d1 —> 3\n",
            "d2 —> 1\n",
            "d3 —> 2\n",
            "\n",
            "Best Matching Document: d1\n",
            "Best Match Text: wireless headphones with active noise cancellation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7NKyGYvwXrCB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}